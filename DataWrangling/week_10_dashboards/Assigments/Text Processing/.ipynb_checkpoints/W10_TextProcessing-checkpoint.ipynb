{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of this tutorial has been extracted from the assignment prepared by **Marijn Schraagen** for the data analytics course that I taught last year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDoc = 'Data has been coined \"The oil of the 21st century\".'\n",
    "myDoc += ' Businesses and organizations have realized that in order to thrive in the data driven economy,'\n",
    "myDoc += ' have to adopt modern data management solutions that will allow them to innovate and generate high-quality'\n",
    "myDoc += ' added value services.'\n",
    "# TODO: Tokenize the documnet myDoc and display the set of tokens .. \n",
    "# consider the default tokenization (tokens are separated by spaces and special characters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tokenize the documnet myDoc and display the set of tokens .. \n",
    "# consider the tokens are separated by spaces and special characters (hyphen should be considered as special character).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = \"It is important to be very pythonly while you are pythoning with python.\"\n",
    "sentence += \" All pythoners have pythoned poorly at least once.\"\n",
    "# TODO: run porter's stemmer to find the root of each of the words in the sentence\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Edit Distance (Levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ldist(s, t):\n",
    "    \"\"\" \n",
    "        iterative_levenshtein(s, t) -> ldist\n",
    "        ldist is the Levenshtein distance between the strings \n",
    "        s and t.\n",
    "        For all i and j, dist[i,j] will contain the Levenshtein \n",
    "        distance between the first i characters of s and the \n",
    "        first j characters of t\n",
    "    \"\"\"\n",
    "\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    dist = [[0 for x in range(cols)] for x in range(rows)]\n",
    "\n",
    "    # source prefixes can be transformed into empty strings \n",
    "    # by deletions:\n",
    "    for i in range(1, rows):\n",
    "        dist[i][0] = i\n",
    "\n",
    "    # target prefixes can be created from an empty source string\n",
    "    # by inserting the characters\n",
    "    for i in range(1, cols):\n",
    "        dist[0][i] = i\n",
    "        \n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                c = 0\n",
    "            else:\n",
    "                c = 2\n",
    "            dist[row][col] = min(dist[row-1][col] + 1,       # deletion\n",
    "                                 dist[row][col-1] + 1,       # insertion\n",
    "                                 dist[row-1][col-1] + c)     # substitution\n",
    "\n",
    "    for r in range(rows):\n",
    "        print(dist[r])\n",
    "    \n",
    " \n",
    "    return dist[row][col]\n",
    "\n",
    "print(ldist(\"Intention\", \"execution\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify the ldist function to accept a new parameter which specifies the substitution cost\n",
    "# Test your implementation with the call mod_ldist(\"Intention\", \"execution\", 1) -- You should get 5\n",
    "# and mod_ldist(\"Intention\", \"execution\", 2) -- You should get 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Now consider the following words: \n",
    "w1 = 'AGGCTATCACCTGACCTCCAGGCCGATGCCC'\n",
    "w2 = 'TAGCTATCACGACCGCGGTCGATTTGCCCGAC'\n",
    "w3 = 'AGCTATCACGACCGCGGTCGATTTGCCCGACCC'\n",
    "# Compute the paiwise minimum edit edit distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = 'Businesses and organizations have realized that in order to thrive in the data driven economy,'\n",
    "D1 += ' have to adopt modern data management solutions that will allow them to innovate and generate high-quality'\n",
    "D1 += ' added value services.'\n",
    "D2 = 'Successful organizations most likely have data management technology powering every process.'\n",
    "D3 = 'Data management solutions and systems that help to deal with and manage the full data lifecycle needs of your company.'\n",
    "D3 += ' Manage the development and execution of architectures, policies, practices and procedures with the data management systems found on bobsguide.'\n",
    "# TODO: Create a dictionary that contains the term and its frequency in the document (stop words shouldn't be included). \n",
    "# Compute the normalized document-term matrix and display the values for the terms\n",
    "# organizations, data, management, solutions, systems\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the D1, D2, D3, compute the TF-IDF matrix for the terms in the previous example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building language identification classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to use what we learned during the text processing lecture to build a language identification classifier, for four languages Dutch, English, German and French. Trigrams (3-shingles) proved to be good measure that can be used to compare blocks of text based on their local structure, which is a good indicator of the language used. \n",
    "\n",
    "To build the model, a corpus is provided with 28 documents from Wikipedia, 20 training documents (5 per language) and 8 test documents (2 per language). A file with the basic structure of the code is provided as an attachment ( da_textmining_2020.r ). The algorithm you should implement is as follows:\n",
    "\n",
    "1. Read the training data from the text files, perform data cleaning and split into word/tokens.\n",
    "2. Compute a vector of all **trigrams** for each language.\n",
    "3. Compute a **frequency table** and store the top 300 most frequent trigrams for each language.\n",
    "4. For each individual test document, compute the 300 most frequent trigrams.\n",
    "5. Implement a rank comparison algorithm to compare the 300 test trigrams for each test document to each of the four language models.\n",
    "6. For each test document, present the results of the comparison in sorted order to the user.\n",
    "\n",
    "Test your implementation on the provided test corpus. Based on the numbers, which language is closest to Dutch,\n",
    "and which language is furthest away? \n",
    "\n",
    "Analyze how much data you need. Reduce the number of words used for the construction of the train and test language models gradually and check the performance. Extract words from the middle of the document (for example starting at the 100th word) to reduce influence of any Wikipedia-specific phrases. Make a table showing the number or words used vs. the number of\n",
    "classification errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation details\n",
    "Read the training data from the text files, perform data cleaning and split into word/tokens. \n",
    "For reading files the library readr provides the read_file function. After loading the file, perform the following preprocessing steps:\n",
    "* convert to lowercase\n",
    "* strip newline characters \\n \n",
    "* strip everything that is not an alphanumeric character, a space or an apostrophe (' ). You can use regular expressions for this\n",
    "* convert multiple spaces to one using the gsub function\n",
    "* extract a list of words from the document\n",
    "* Store all words from all documents of one language together in a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a function that receives a filename, reads all of its contents, converts all letters to lowercase and \n",
    "# removes the '\\r', '\\n' and any extra spaces and returns the list of words \n",
    "def get_words (filename):    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute a vector of all trigrams for each language:**\n",
    "Write a function that computes all trigrams for a single word. Then, apply this function on the full list of words for a language. The function to compute trigrams should first add the boundary character _ (underscore) at the start and end\n",
    "of the word. Then, using a for loop over the length of the word, extract all trigrams (3-shingles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigrams(word):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute a frequency table and store the top 300 most frequent trigrams for each language:** Write a function to compute a frequency table. Sort (most frequent on top) and select the top 300 most frequent trigrams. Note: convert the result to a data frame to enable rank comparison in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def get_top300 (words):\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For each individual test document, compute the 300 most frequent trigrams:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the comparison score between reference and test models\n",
    "# If a trigram gram exists in both models, add the difference between the rank \n",
    "# of the trigram in both models to the comaprison score\n",
    "# If a trigram exists in the test model but not in the ref. model add a penalty of 500 the comparison score\n",
    "def compare_model(model_ref, model_test):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the names of the training files and read the contents of each file. \n",
    "Create a reference model for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# TODO: Get the name of the files in the following path\n",
    "\n",
    "path = 'txt/Training/'\n",
    "\n",
    "\n",
    "# Now: create a dictionary for each class of names to store the names of the files \n",
    "# that represents a specific language \n",
    "# read the contents of all the document from a given language into a single list or vector \n",
    "# (you will need 4 lists or vectors)\n",
    "# Create the reference models of the languages by storing the top 300 frequent trigrams of each language\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same pipeline as before (preprocessing, create a word list, compute trigrams, select top 300, convert\n",
    "to data frame) but now for an individual test file (instead of all files for a language as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-Recht.txt\n",
      "{'de': 57210, 'nl': 98233, 'en': 112190, 'fr': 113559}\n",
      "de-Wirtschaftswissenschaft.txt\n",
      "{'de': 62659, 'nl': 99068, 'en': 110925, 'fr': 112940}\n",
      "en-Economics.txt\n",
      "{'en': 59830, 'fr': 95597, 'nl': 108235, 'de': 110558}\n",
      "en-Law.txt\n",
      "{'en': 63615, 'fr': 96542, 'nl': 108886, 'de': 109644}\n",
      "fr-Droit.txt\n",
      "{'fr': 53981, 'en': 102131, 'de': 112984, 'nl': 116002}\n",
      "fr-Sciences_economiques.txt\n",
      "{'fr': 50437, 'en': 95457, 'de': 108292, 'nl': 111584}\n",
      "nl-Economie.txt\n",
      "{'nl': 59802, 'de': 94938, 'en': 101698, 'fr': 112395}\n",
      "nl-Recht.txt\n",
      "{'nl': 65473, 'de': 98252, 'en': 104456, 'fr': 113297}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Get the name of the files in the following path\n",
    "test_path = 'txt/Testing/'\n",
    "# TODO: for each file, find the most frequest 300 trigrams and compare the model to the reference models of the languages\n",
    "# Print the comparison score in ascending order to see which language is closest to the test file \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output that you will get should be close to:\n",
    "\n",
    "de-Recht.txt\n",
    "\n",
    "{'de': 57210, 'nl': 98233, 'en': 112190, 'fr': 113559}\n",
    "\n",
    "de-Wirtschaftswissenschaft.txt\n",
    "\n",
    "{'de': 62659, 'nl': 99068, 'en': 110925, 'fr': 112940}\n",
    "\n",
    "en-Economics.txt\n",
    "\n",
    "{'en': 59830, 'fr': 95597, 'nl': 108235, 'de': 110558}\n",
    "\n",
    "en-Law.txt\n",
    "\n",
    "{'en': 63615, 'fr': 96542, 'nl': 108886, 'de': 109644}\n",
    "\n",
    "fr-Droit.txt\n",
    "\n",
    "{'fr': 53981, 'en': 102131, 'de': 112984, 'nl': 116002}\n",
    "\n",
    "fr-Sciences_economiques.txt\n",
    "\n",
    "{'fr': 50437, 'en': 95457, 'de': 108292, 'nl': 111584}\n",
    "\n",
    "nl-Economie.txt\n",
    "\n",
    "{'nl': 59802, 'de': 94938, 'en': 101698, 'fr': 112395}\n",
    "\n",
    "nl-Recht.txt\n",
    "\n",
    "{'nl': 65473, 'de': 98252, 'en': 104456, 'fr': 113297}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
