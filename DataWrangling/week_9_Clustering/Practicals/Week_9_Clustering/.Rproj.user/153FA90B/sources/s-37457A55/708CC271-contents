---
title: "Week9_Assigment 2"
author: "Hans Franke"
output: rmarkdown::github_document
---


```{r load library}
library(MASS) # make sure to load mass before tidyverse to avoid conflicts!
library(tidyverse)
#install.packages("patchwork")
library(patchwork)
library(ggdendro)
library(mclust)
```

In this practical, we will apply model-based clustering on a dataset of bank note measurements. The data is built into the mclust package and can be loaded as a tibble by running the following code:
```{r import Banknote}

df <- as_tibble(banknote)
head(df)
```


# Data exploration

Read the help file of the banknote dataset to understand what it’s all about.

Create a scatter plot of the left (x-axis) and right (y-axis) measurements on the dataset. Map the Status column to colour. Jitter the points to avoid overplotting. Are the classes easy to distinguish based on these features?
```{r scatterplot left x right}

ggplot(df, aes(Left, Right, color=Status))+geom_jitter()
#We can see there is overlaping in classes considering only this 2 features, so no clear distinguishing
```

From now on, we will assume that we don’t have the labels. Remove the Status column from the dataset.

```{r removing labels}
df_unlabel <- df[,2:7]
head(df_unlabel)
```

Create density plots for all columns in the dataset. Which single feature is likely to be best for clustering?

```{r}

(ggplot(df_unlabel )+
     geom_density(aes(x=Length)) ) +

        (ggplot(df_unlabel )+
          geom_density(aes(x=Left)) ) +
  
            (ggplot(df_unlabel )+
                geom_density(aes(x=Right)) ) +
  
              (ggplot(df_unlabel )+
                geom_density(aes(x=Bottom)) ) +
  
                (ggplot(df_unlabel )+
                geom_density(aes(x=Top)) ) +
  
                        (ggplot(df_unlabel )+
                geom_density(aes(x=Diagonal)) ) 

#Diagonal seems to have a non-normal distribution, so this can explain Clusters better, probably there is two normal distributions (clusters)
```
```{r check feature}
(ggplot(df )+
     geom_density(aes(x=Diagonal, color=Status)) ) + theme_minimal()

#As we imagine, indeed there is 2 normal distribution explaining the classes
```

```{r}
(ggplot(df )+
     geom_density(aes(x=Length, color=Status)) ) + theme_minimal()
```

# Univariate model-based clustering

```{r}
class <- df_unlabel[,6]
fit <- Mclust(class)
summary(fit)

plot(fit, what="BIC")
#fit_E_2$parameters
```



Use Mclust to perform model-based clustering with 2 clusters on the feature you chose. Assume equal variances. Name the model object fit_E_2. What are the means and variances of the clusters?

```{r}
class <- df_unlabel[,6]
fit_E_2 <- Mclust(class, modelNames = c("E"), G=2)
summary(fit_E_2)

fit_E_2$parameters
```


Use the formula from the slides and the model’s log-likelihood (fit_E_2$loglik) to compute the BIC for this model. Compare it to the BIC stored in the model object (fit_E_2$bic). Explain how many parameters (m) you used and which parameters these are.

```{r}
#stored
fit_E_2$bic

#parameters
fit_E_2$parameters
#k = 2 (2 clusters)
#p = 1 (variable)
#the parameters are (1 class probability pi, 2 means, and 1 variance)

#from book (we can see 4 parameters)
fit_E_2$loglik
```


Plot the model-implied density using the plot() function. Afterwards, add rug marks of the original data to the plot using the rug() function from the base graphics system.

```{r}
plot(fit_E_2, what="BIC")
plot(fit_E_2, what="density")
# add the observations using rug marks
rug(df_unlabel %>% pull(Diagonal))
```

Use Mclust to perform model-based clustering with 2 clusters on this feature again, but now assume unequal variances. Name the model object fit_V_2. What are the means and variances of the clusters? Plot the density again and note the differences.

```{r}
class <- df_unlabel[,6]
fit_V_2 <- Mclust(class, modelNames = c("V"), G=2)
summary(fit_V_2)

fit_V_2$parameters
```
```{r}

plot(fit_V_2, what="BIC")
plot(fit_V_2, what="density")
# add the observations using rug marks
rug(df_unlabel %>% pull(Diagonal))

```

How many parameters does this model have? Name them.

```{r}
fit_V_2$parameters

# 1 class probability (pi)
# 2 means
# 2 variances
```

According to the deviance, which model fits better?
```{r}
#deviance = -2 * log(l)
-2*fit_E_2$loglik
-2*fit_V_2$loglik
```


According to the BIC, which model is better?

```{r}
fit_E_2$bic
fit_V_2$bic
```

# Multivariate model-based clustering

We will now use all available information in the dataset to cluster the observations.


Use Mclust with all 6 features to perform clustering. Allow all model types (shapes), and from 1 to 9 potential clusters. What is the optimal model based on the BIC?

```{r}
fit_multi <- Mclust(df_unlabel)
summary(fit_multi)
plot(fit_multi, what="BIC")
```
```{r}
fit_multi$parameters
# 3 clusters * 6 variables = 18 mean parameters
```


How many mean parameters does this model have?

```{r}
fit_multi$parameters$mean
# 3 clusters * 6 variables = 18 mean parameters

```

Run a 2-component VVV model on this data. Create a matrix of bivariate contour (“density”) plots using the plot() function. Which features provide good component separation? Which do not?

```{r}
fit_multi_VVV <- Mclust(df_unlabel, G=2, modelNames = c("VVV"))

plot(fit_multi_VVV, what="density")
```

Create a scatter plot just like the first scatter plot in this tutorial, but map the estimated class assignments to the colour aesthetic. Map the uncertainty (part of the fitted model list) to the size aesthetic, such that larger points indicate more uncertain class assignments. Jitter the points to avoid overplotting. What do you notice about the uncertainty?

```{r}
ggplot(df, aes(Left, Right, color=fit_multi_VVV$classification, size=fit_multi_VVV$uncertainty))+geom_jitter()
```

```{r}
#now with the best separation between features
ggplot(df, aes(Top, Diagonal, color=fit_multi_VVV$classification, size=fit_multi_VVV$uncertainty))+geom_jitter()
```

