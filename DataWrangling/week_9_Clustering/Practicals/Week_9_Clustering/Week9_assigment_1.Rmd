---
title: "R Notebook"
output: rmarkdown::github_document
---

```{r}
library(tidyverse)
```

```{r}
distances <- dist(faithful, method = "euclidean")
result <- hclust(distances, method = "average")
```

```{r}
#install.packages("ggdendro")
library(ggdendro)
ggdendrogram(result)

```

```{r}
df_kmeans <- kmeans(faithful, centers=2)
str(df_kmeans)

df_kmeans$cluster
df_kmeans$centers

#install.packages("factoextra")
library("factoextra")
fviz_cluster(df_kmeans, data = faithful)
```
## Look for the best number of clusters

```{r}

fviz_nbclust(faithful, FUN=hcut, method = "wss") #elbow method
```
```{r}
fviz_nbclust(faithful, FUN=hcut, method = "silhouette") #silhouette
```


```{r}
library(MASS) # make sure to load mass before tidyverse to avoid conflicts!
library(tidyverse)
#install.packages("patchwork")
library(patchwork)
library(ggdendro)
```

# Hierarchical and k-means clustering Introduction

We use the following packages:

library(MASS) # make sure to load mass before tidyverse to avoid conflicts!
library(tidyverse)
library(patchwork)
library(ggdendro)

In this practical, we will apply hierarchical and k-means clustering to two synthetic datasets. The data can be generated by running the code below.

Try to understand what is happening as you run each line of the code below

```{r}
# randomly generate bivariate normal data
set.seed(123)
sigma      <- matrix(c(1, .5, .5, 1), 2, 2)
sim_matrix <- mvrnorm(n = 100, mu = c(5, 5), Sigma = sigma)
colnames(sim_matrix) <- c("x1", "x2")

# change to a data frame (tibble) and add a cluster label column
sim_df <- 
  sim_matrix %>% 
  as_tibble() %>%
  mutate(class = sample(c("A", "B", "C"), size = 100, replace = TRUE))

# Move the clusters to generate separation
sim_df_small <- 
  sim_df %>%
  mutate(x2 = case_when(class == "A" ~ x2 + .5,
                        class == "B" ~ x2 - .5,
                        class == "C" ~ x2 + .5),
         x1 = case_when(class == "A" ~ x1 - .5,
                        class == "B" ~ x1 - 0,
                        class == "C" ~ x1 + .5))
sim_df_large <- 
  sim_df %>%
  mutate(x2 = case_when(class == "A" ~ x2 + 2.5,
                        class == "B" ~ x2 - 2.5,
                        class == "C" ~ x2 + 2.5),
         x1 = case_when(class == "A" ~ x1 - 2.5,
                        class == "B" ~ x1 - 0,
                        class == "C" ~ x1 + 2.5))
```

Prepare two unsupervised datasets by removing the class feature

```{r}
sim_df_small_un <- sim_df_small[,1:2]
sim_df_large_un <- sim_df_large[,1:2]
```


For each of these datasets, create a scatterplot. Combine the two plots into a single frame (look up the “patchwork” package to see how to do this!) What is the difference between the two datasets?
```{r}
# at first plot we can imagine as a single cluster, in the second we can think of 3 clusters
g1 <- ggplot(sim_df_small_un, aes(x1,x2)) + geom_point() + ylim(0,10) + xlim(0,10)
g2 <- ggplot(sim_df_large_un, aes(x1,x2)) + geom_point() +ylim(0,10) + xlim(0,10)
g1 + g2
```
```{r}
#look if this is true!
sim_kmeans <- kmeans(sim_df_large_un, centers=3)

fviz_cluster(sim_kmeans, data = sim_df_large_un)
```
```{r}
#look if this is true!
sim_kmeans <- kmeans(sim_df_small_un, centers=1)

fviz_cluster(sim_kmeans, data = sim_df_small_un)
```
```{r}
#Original DF
sim_kmeans <- kmeans(sim_df[,1:2], centers=3)

fviz_cluster(sim_kmeans, data = sim_df[,1:2])
```


# Hierarchical clustering
Run a hierarchical clustering on these datasets and display the result as dendrograms. Use euclidian distances and the complete agglomeration method. Make sure the two plots have the same y-scale. What is the difference between the dendrograms? (Hint: functions you’ll need are hclust, ggdendrogram, and ylim)

```{r}
#Small dataset
distances <- dist(sim_df_small_un, method = "euclidean")
result_com_eu <- hclust(distances, method = "complete")
g1 <- ggdendrogram(result_com_eu) + ylim(0,10) + labs(title = "Small Dataset")

#large dataset
distances <- dist(sim_df_large_un, method = "euclidean")
result <- hclust(distances, method = "complete")
g2 <- ggdendrogram(result) + ylim(0,10) + labs(title = "Large Dataset")

g1 + g2

#in the small dataset the distances are lesser than on large, so the points are closer to eachother, as we see in previous experiments. Max height = max distance = 5,5 on smalldf, and 10 in largedf.
```

For the dataset with small differences, also run a complete agglomeration hierarchical cluster with manhattan distance.
```{r}
#Small dataset
distances <- dist(sim_df_small_un, method = "manhattan")
result_com_man <- hclust(distances, method = "complete")
g1 <- ggdendrogram(result_com_man) + ylim(0,10) + labs(title = "Small Dataset")
g1

#now we see that the average distance increase.
```

Use the cutree() function to obtain the cluster assignments for three clusters and compare the cluster assignments to the 3-cluster euclidian solution. Do this comparison by creating two scatter plots with cluster assignment mapped to the colour aesthetic. Which difference do you see?

```{r}
man <- cutree(result_com_man, k=3) 
euc <- cutree(result_com_eu, k=3)

sim_df_small_un  <- sim_df_small_un %>%
  mutate("man" = man, #assign the classes from manhattan distance
         "euc" = euc) #assign the classes from euclidean distance

g1 <- ggplot(sim_df_small_un, aes(x1,x2, color=man)) + geom_point() + labs(title= "3-Clusters Manhattan Distances")
g2 <- ggplot(sim_df_small_un, aes(x1,x2, color=euc)) + geom_point() + labs(title= "3-Clusters Euclidean Distances")

g1 + g2

#we see the boundaries of the classes mostly in the middle-points is different
```


# K-means clustering
Create k-means clusterings with 2, 3, 4, and 6 classes on the large difference data. Again, create coloured scatter plots for these clusterings.

```{r}
k2 <- kmeans(sim_df_large_un, centers=2)
k3 <- kmeans(sim_df_large_un, centers=3)
k4 <- kmeans(sim_df_large_un, centers=4)
k6 <- kmeans(sim_df_large_un, centers=6)

sim_df_large_un_kmeans <- sim_df_large_un %>%
  mutate("k2" = k2$cluster,
         "k3" = k3$cluster,
         "k4" = k4$cluster,
         "k6" = k6$cluster,
         )

(ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k2))+geom_point() + theme_classic()) +
  (ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k3))+geom_point() + theme_classic()) +
   (ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k4))+geom_point() + theme_classic()) +
    (ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k6))+geom_point() + theme_classic()) 
```
```{r}
k2 <- kmeans(sim_df_large_un, centers=2)
k3 <- kmeans(sim_df_large_un, centers=3)
k4 <- kmeans(sim_df_large_un, centers=4)
k6 <- kmeans(sim_df_large_un, centers=6)

sim_df_large_un_kmeans <- sim_df_large_un %>%
  mutate("k2" = k2$cluster,
         "k3" = k3$cluster,
         "k4" = k4$cluster,
         "k6" = k6$cluster,
         )

(ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k2))+geom_point() + theme_classic()) +
  (ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k3))+geom_point() + theme_classic()) +
   (ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k4))+geom_point() + theme_classic()) +
    (ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k6))+geom_point() + theme_classic()) 
```

Do the same thing again a few times. Do you see the same results every time? where do you see differences?
```{r}
#Yes as we assign random values as start position the classes can change, specialy at large the number of clusters.
```


Find a way online to perform bootstrap stability assessment for the 3 and 6-cluster solutions.


