fit_E_2 <- Mclust(class, modelNames = c("E"), G=2)
summary(fit_E_2)
fit_E_2$parameters
#stored
fit_E_2$bic
#parameters
fit_E_2$parameters
#k = 2 (2 clusters)
#p = 1 (variable)
#the parameters are (1 class probability pi, 2 means, and 1 variance)
#from book (we can see 4 parameters)
-2*fit_E_2$loglik + log(nrow(df))*4
plot(fit_E_2, what="BIC")
plot(fit_E_2, what="density")
# add the observations using rug marks
rug(df_unlabel %>% pull(Diagonal))
class <- df_unlabel[,6]
fit_V_2 <- Mclust(class, modelNames = c("V"), G=2)
summary(fit_V_2)
fit_V_2$parameters
plot(fit_V_2, what="BIC")
plot(fit_V_2, what="density")
# add the observations using rug marks
rug(df_unlabel %>% pull(Diagonal))
fit_V_2$parameters
# 1 class probability (pi)
# 2 means
# 2 variances
#deviance = -2 * log(l)
-2*fit_E_2$loglik
-2*fit_V_2$loglik
fit_E_2$bic
fit_V_2$bic
fit_multi <- Mclust(df_unlabel)
summary(fit_multi)
plot(fit_multi, what="BIC")
fit_multi$parameters
# 3 clusters * 6 variables = 18 mean parameters
fit_multi$parameters$mean
# 3 clusters * 6 variables = 18 mean parameters
fit_multi_VVV <- Mclust(df_unlabel, G=2, modelNames = c("VVV"))
#summary(fit_multi_VVV)
plot(fit_multi_VVV, what="density")
ggplot(df, aes(Left, Right, color=as_factor(fit_multi_VVV$classification), size=fit_multi_VVV$uncertainty))+geom_jitter()
#now with the best separation between features
ggplot(df, aes(Top, Diagonal, color=as_factor(fit_multi_VVV$classification), size=fit_multi_VVV$uncertainty))+geom_jitter()
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("VVV"))
summary(fit_multi_XXX)
library(tidyverse)
distances <- dist(faithful, method = "euclidean")
result <- hclust(distances, method = "average")
#install.packages("ggdendro")
library(ggdendro)
ggdendrogram(result)
df_kmeans <- kmeans(faithful, centers=2)
str(df_kmeans)
df_kmeans$cluster
df_kmeans$centers
#install.packages("factoextra")
library("factoextra")
fviz_cluster(df_kmeans, data = faithful)
fviz_nbclust(faithful, FUN=hcut, method = "wss") #elbow method
fviz_nbclust(faithful, FUN=hcut, method = "silhouette") #silhouette
library(MASS) # make sure to load mass before tidyverse to avoid conflicts!
library(tidyverse)
#install.packages("patchwork")
library(patchwork)
library(ggdendro)
# randomly generate bivariate normal data
set.seed(123)
sigma      <- matrix(c(1, .5, .5, 1), 2, 2)
sim_matrix <- mvrnorm(n = 100, mu = c(5, 5), Sigma = sigma)
colnames(sim_matrix) <- c("x1", "x2")
# change to a data frame (tibble) and add a cluster label column
sim_df <-
sim_matrix %>%
as_tibble() %>%
mutate(class = sample(c("A", "B", "C"), size = 100, replace = TRUE))
# Move the clusters to generate separation
sim_df_small <-
sim_df %>%
mutate(x2 = case_when(class == "A" ~ x2 + .5,
class == "B" ~ x2 - .5,
class == "C" ~ x2 + .5),
x1 = case_when(class == "A" ~ x1 - .5,
class == "B" ~ x1 - 0,
class == "C" ~ x1 + .5))
sim_df_large <-
sim_df %>%
mutate(x2 = case_when(class == "A" ~ x2 + 2.5,
class == "B" ~ x2 - 2.5,
class == "C" ~ x2 + 2.5),
x1 = case_when(class == "A" ~ x1 - 2.5,
class == "B" ~ x1 - 0,
class == "C" ~ x1 + 2.5))
sim_df_small_un <- sim_df_small[,1:2]
sim_df_large_un <- sim_df_large[,1:2]
# at first plot we can imagine as a single cluster, in the second we can think of 3 clusters
g1 <- ggplot(sim_df_small_un, aes(x1,x2)) + geom_point() + ylim(0,10) + xlim(0,10)+ggtitle("Small")
g2 <- ggplot(sim_df_large_un, aes(x1,x2)) + geom_point() +ylim(0,10) + xlim(0,10)+ggtitle("Large")
g1 + g2
#look if this is true!
sim_kmeans <- kmeans(sim_df_large_un, centers=3)
fviz_cluster(sim_kmeans, data = sim_df_large_un)
#look if this is true!
sim_kmeans <- kmeans(sim_df_small_un, centers=1)
fviz_cluster(sim_kmeans, data = sim_df_small_un)
#Original DF
sim_kmeans <- kmeans(sim_df[,1:2], centers=3)
fviz_cluster(sim_kmeans, data = sim_df[,1:2])
#Small dataset
distances <- dist(sim_df_small_un, method = "euclidean")
result_com_eu <- hclust(distances, method = "complete")
g1 <- ggdendrogram(result_com_eu) + ylim(0,10) + labs(title = "Small Dataset")
#large dataset
distances <- dist(sim_df_large_un, method = "euclidean")
result <- hclust(distances, method = "complete")
g2 <- ggdendrogram(result) + ylim(0,10) + labs(title = "Large Dataset")
g1 + g2
#in the small dataset the distances are lesser than on large, so the points are closer to eachother, as we see in previous experiments. Max height = max distance = 5,5 on smalldf, and 10 in largedf.
#Small dataset
distances <- dist(sim_df_small_un, method = "manhattan")
result_com_man <- hclust(distances, method = "complete")
g1 <- ggdendrogram(result_com_man) + ylim(0,10) + labs(title = "Small Dataset")
g1
#now we see that the average distance increase.
man <- cutree(result_com_man, k=3)
euc <- cutree(result_com_eu, k=3)
sim_df_small_un  <- sim_df_small_un %>%
mutate("man" = man, #assign the classes from manhattan distance
"euc" = euc) #assign the classes from euclidean distance
g1 <- ggplot(sim_df_small_un, aes(x1,x2, color=man)) + geom_point() + labs(title= "3-Clusters Manhattan Distances")
g2 <- ggplot(sim_df_small_un, aes(x1,x2, color=euc)) + geom_point() + labs(title= "3-Clusters Euclidean Distances")
g1 + g2
#we see the boundaries of the classes mostly in the middle-points is different
k2 <- kmeans(sim_df_large_un, centers=2)
k3 <- kmeans(sim_df_large_un, centers=3)
k4 <- kmeans(sim_df_large_un, centers=4)
k6 <- kmeans(sim_df_large_un, centers=6)
sim_df_large_un_kmeans <- sim_df_large_un %>%
mutate("k2" = k2$cluster,
"k3" = k3$cluster,
"k4" = k4$cluster,
"k6" = k6$cluster,
)
(ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k2))+geom_point() + theme_classic()) +
(ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k3))+geom_point() + theme_classic()) +
(ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k4))+geom_point() + theme_classic()) +
(ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k6))+geom_point() + theme_classic())
k2 <- kmeans(sim_df_large_un, centers=2)
k3 <- kmeans(sim_df_large_un, centers=3)
k4 <- kmeans(sim_df_large_un, centers=4)
k6 <- kmeans(sim_df_large_un, centers=6)
sim_df_large_un_kmeans <- sim_df_large_un %>%
mutate("k2" = k2$cluster,
"k3" = k3$cluster,
"k4" = k4$cluster,
"k6" = k6$cluster,
)
(ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k2))+geom_point() + theme_classic()) +
(ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k3))+geom_point() + theme_classic()) +
(ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k4))+geom_point() + theme_classic()) +
(ggplot(sim_df_large_un_kmeans, aes(x1,x2, color=k6))+geom_point() + theme_classic())
#Yes as we assign random values as start position the classes can change, specialy at large the number of clusters.
#install.packages("fpc")
library("fpc")
#set desire number of clusters
kbest.p<- 6
#   called cboot.hclust.
cboot.hclust <- clusterboot(sim_df_large_un, clustermethod=hclustCBI,
method="ward.D", k=kbest.p)
#alternative version
cboot.kmeansCBI <- clusterboot(sim_df_large_un, clustermethod=kmeansCBI, k=kbest.p)
#   The results of the clustering are in
#   cboot.hclust$result. The output of the hclust()
#   function is in cboot.hclust$result$result.
#
#   cboot.hclust$result$partition returns a
#   vector of clusterlabels.
groups<-cboot.hclust$result$partition
cboot.hclust$bootmean
cboot.kmeansCBI$bootmean
# We can see that the stability on each cluster (we see that k=5 has the most stability and 3 and 6 the fewer, as we see from previous example)
clusters <- 1:6
stability2 <- cboot.kmeansCBI$bootmean
stability <- cboot.hclust$bootmean
ggplot(data=NULL) + geom_line(aes(x=clusters, y=stability, color='HCLUST')) + geom_line(aes(x=clusters, y=stability2, color="Kmeans")) + xlim(1,6) + ylim(0,1) + labs(title="Stability test with resampling")
#solution from Ayob
boot_3 <- clusterboot(sim_df_large_un, B = 1000, clustermethod = kmeansCBI, k = 3,
count = FALSE)
boot_6 <- clusterboot(sim_df_large_un, B = 1000, clustermethod = kmeansCBI, k = 6,
count = FALSE)
# the average stability is much lower for 6 means than for 3 means:
boot_3$bootmean
boot_6$bootmean
fit_multi_VVV <- Mclust(df_unlabel, G=2, modelNames = c("VVV"))
#summary(fit_multi_VVV)
fit_multi$parameters$df
fit_multi_VVV <- Mclust(df_unlabel, G=2, modelNames = c("VVE"))
#summary(fit_multi_VVV)
fit_multi$parameters$df
plot(fit_multi_VVV, what="density")
fit_multi_VVV <- Mclust(df_unlabel, G=2, modelNames = c("VVV"))
#summary(fit_multi_VVV)
fit_multi$parameters$df
fit_multi_VVV <- Mclust(df_unlabel, G=2, modelNames = c("VVE"))
#summary(fit_multi_VVV)
fit_multi$parameters$df
#plot(fit_multi_VVV, what="density")
fit_multi_VVV <- Mclust(df_unlabel, G=2, modelNames = c("VVV"))
#summary(fit_multi_VVV)
fit_multi$parameters
fit_multi_VVV <- Mclust(df_unlabel, G=2, modelNames = c("VVE"))
#summary(fit_multi_VVV)
fit_multi$parameters
#plot(fit_multi_VVV, what="density")
fit_multi_VVV <- Mclust(df_unlabel, G=2, modelNames = c("VVV"))
summary(fit_multi_VVV)
fit_multi$parameters
fit_multi_VVV <- Mclust(df_unlabel, G=2, modelNames = c("VVE"))
#summary(fit_multi_VVV)
fit_multi$parameters
#plot(fit_multi_VVV, what="density")
fit_multi_VVV <- Mclust(df_unlabel, G=2, modelNames = c("VVV"))
summary(fit_multi_VVV)
fit_multi$df
fit_multi_VVV <- Mclust(df_unlabel, G=2, modelNames = c("VVE"))
#summary(fit_multi_VVV)
fit_multi$df
#plot(fit_multi_VVV, what="density")
fit_multi_VVV <- Mclust(df_unlabel, G=2, modelNames = c("VVV"))
#summary(fit_multi_VVV)
fit_multi$df
fit_multi_VVV <- Mclust(df_unlabel, G=2, modelNames = c("VVE"))
#summary(fit_multi_VVV)
fit_multi$df
#plot(fit_multi_VVV, what="density")
fit_multi_VVV <- Mclust(df_unlabel[,1:2], G=2, modelNames = c("VVV"))
#summary(fit_multi_VVV)
fit_multi$df
fit_multi_VVV <- Mclust(df_unlabel1:2], G=2, modelNames = c("VVE"))
fit_multi_VVV <- Mclust(df_unlabel[,1:2], G=2, modelNames = c("VVV"))
#summary(fit_multi_VVV)
fit_multi$df
fit_multi_VVV <- Mclust(df_unlabel[1:2], G=2, modelNames = c("VVE"))
#summary(fit_multi_VVV)
fit_multi$df
fit_multi_VVV <- Mclust(df_unlabel[1:2], G=2, modelNames = c("EEE"))
#summary(fit_multi_VVV)
fit_multi$df
#plot(fit_multi_VVV, what="density")
fit_multi_VVV <- Mclust(df_unlabel[,c(Left,Right,Diagonal)], G=2, modelNames = c("VVV"))
df <- df_unlabel[,1:2]
fit_multi_VVV <- Mclust(df, G=2, modelNames = c("VVV"))
#summary(fit_multi_VVV)
fit_multi$df
fit_multi_VVV <- Mclust(df_unlabel[1:2], G=2, modelNames = c("VVE"))
#summary(fit_multi_VVV)
fit_multi$df
fit_multi_VVV <- Mclust(df_unlabel[1:2], G=2, modelNames = c("EEE"))
#summary(fit_multi_VVV)
fit_multi$df
#plot(fit_multi_VVV, what="density")
df <- df_unlabel[,1:2]
df
fit_multi_VVV <- Mclust(df, G=2, modelNames = c("VVV"))
#summary(fit_multi_VVV)
fit_multi$df
fit_multi_VVV <- Mclust(df_unlabel[1:2], G=2, modelNames = c("VVE"))
#summary(fit_multi_VVV)
fit_multi$df
fit_multi_VVV <- Mclust(df_unlabel[1:2], G=2, modelNames = c("EEE"))
#summary(fit_multi_VVV)
fit_multi$df
#plot(fit_multi_VVV, what="density")
df <- df_unlabel[,1:3]
df
fit_multi_VVV <- Mclust(df, G=2, modelNames = c("VVV"))
#summary(fit_multi_VVV)
fit_multi$df
fit_multi_VVV <- Mclust(df_unlabel[1:2], G=2, modelNames = c("VVE"))
#summary(fit_multi_VVV)
fit_multi$df
fit_multi_VVV <- Mclust(df_unlabel[1:2], G=2, modelNames = c("EEE"))
#summary(fit_multi_VVV)
fit_multi$df
#plot(fit_multi_VVV, what="density")
df <- df_unlabel[,1:3]
df
fit_multi_VVV <- Mclust(df, G=2, modelNames = c("VVV"))
#summary(fit_multi_VVV)
fit_multi$df
fit_multi_VVV <- Mclust(df, G=2, modelNames = c("VVE"))
#summary(fit_multi_VVV)
fit_multi$df
fit_multi_VVV <- Mclust(df, G=2, modelNames = c("EEE"))
#summary(fit_multi_VVV)
fit_multi$df
#plot(fit_multi_VVV, what="density")
df <- df_unlabel[,1:3]
fit_multi_VVV <- Mclust(df, G=2, modelNames = c("VVV"))
fit_multi$df
fit_multi_VVE <- Mclust(df, G=2, modelNames = c("VVE"))
fit_multi$df
fit_multi_EEE <- Mclust(df, G=2, modelNames = c("EEE"))
fit_multi$df
#plot(fit_multi_VVV, what="density")
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("VVV"))
summary(fit_multi_XXX)
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("VVV"))
fit_multi_XXX$parameters$df
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("EEE"))
fit_multi_XXX$parameters$df
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("VVE"))
fit_multi_XXX$parameters$df
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("VVV"))
fit_multi_XXX$parameters$df
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("EEE"))
fit_multi_XXX$parameters$df
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("VVE"))
fit_multi_XXX$parameters$df
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("VVV"))
fit_multi_XXX$parameters$df
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("EEE"))
fit_multi_XXX$parameters$df
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("VVE"))
fit_multi_XXX$parameters$df
library(MASS) # make sure to load mass before tidyverse to avoid conflicts!
library(tidyverse)
#install.packages("patchwork")
library(patchwork)
library(ggdendro)
library(mclust)
df <- as_tibble(banknote)
head(df)
ggplot(df, aes(Left, Right, color=Status))+geom_jitter()
#We can see there is overlaping in classes considering only this 2 features, so no clear distinguishing
df_unlabel <- df[,2:7]
head(df_unlabel)
(ggplot(df_unlabel )+
geom_density(aes(x=Length)) ) +
(ggplot(df_unlabel )+
geom_density(aes(x=Left)) ) +
(ggplot(df_unlabel )+
geom_density(aes(x=Right)) ) +
(ggplot(df_unlabel )+
geom_density(aes(x=Bottom)) ) +
(ggplot(df_unlabel )+
geom_density(aes(x=Top)) ) +
(ggplot(df_unlabel )+
geom_density(aes(x=Diagonal)) )
#Diagonal seems to have a non-normal distribution, so this can explain Clusters better, probably there is two normal distributions (clusters)
(ggplot(df )+
geom_density(aes(x=Diagonal, color=Status)) ) + theme_minimal()
#As we imagine, indeed there is 2 normal distribution explaining the classes
(ggplot(df )+
geom_density(aes(x=Length, color=Status)) ) + theme_minimal()
class <- df_unlabel[,6]
fit <- Mclust(class)
summary(fit)
plot(fit, what="BIC")
#fit_E_2$parameters
class <- df_unlabel[,6]
fit_E_2 <- Mclust(class, modelNames = c("E"), G=2)
summary(fit_E_2)
fit_E_2$parameters
#stored
fit_E_2$bic
#parameters
fit_E_2$parameters
#k = 2 (2 clusters)
#p = 1 (variable)
#the parameters are (1 class probability pi, 2 means, and 1 variance)
#from book (we can see 4 parameters)
-2*fit_E_2$loglik + log(nrow(df))*4
plot(fit_E_2, what="BIC")
plot(fit_E_2, what="density")
# add the observations using rug marks
rug(df_unlabel %>% pull(Diagonal))
class <- df_unlabel[,6]
fit_V_2 <- Mclust(class, modelNames = c("V"), G=2)
summary(fit_V_2)
fit_V_2$parameters
plot(fit_V_2, what="BIC")
plot(fit_V_2, what="density")
# add the observations using rug marks
rug(df_unlabel %>% pull(Diagonal))
fit_V_2$parameters
# 1 class probability (pi)
# 2 means
# 2 variances
#deviance = -2 * log(l)
-2*fit_E_2$loglik
-2*fit_V_2$loglik
fit_E_2$bic
fit_V_2$bic
fit_multi <- Mclust(df_unlabel)
summary(fit_multi)
plot(fit_multi, what="BIC")
fit_multi$parameters
# 3 clusters * 6 variables = 18 mean parameters
fit_multi$parameters$mean
# 3 clusters * 6 variables = 18 mean parameters
df <- df_unlabel[,1:3]
fit_multi_VVV <- Mclust(df, G=2, modelNames = c("VVV"))
fit_multi$df
fit_multi_VVE <- Mclust(df, G=2, modelNames = c("VVE"))
fit_multi$df
fit_multi_EEE <- Mclust(df, G=2, modelNames = c("EEE"))
fit_multi$df
#plot(fit_multi_VVV, what="density")
ggplot(df, aes(Left, Right, color=as_factor(fit_multi_VVV$classification), size=fit_multi_VVV$uncertainty))+geom_jitter()
#now with the best separation between features
ggplot(df, aes(Top, Diagonal, color=as_factor(fit_multi_VVV$classification), size=fit_multi_VVV$uncertainty))+geom_jitter()
#now with the best separation between features
ggplot(df, aes(Bottom, Diagonal, color=as_factor(fit_multi_VVV$classification), size=fit_multi_VVV$uncertainty))+geom_jitter()
library(MASS) # make sure to load mass before tidyverse to avoid conflicts!
library(tidyverse)
#install.packages("patchwork")
library(patchwork)
library(ggdendro)
library(mclust)
df <- as_tibble(banknote)
head(df)
ggplot(df, aes(Left, Right, color=Status))+geom_jitter()
#We can see there is overlaping in classes considering only this 2 features, so no clear distinguishing
df_unlabel <- df[,2:7]
head(df_unlabel)
(ggplot(df_unlabel )+
geom_density(aes(x=Length)) ) +
(ggplot(df_unlabel )+
geom_density(aes(x=Left)) ) +
(ggplot(df_unlabel )+
geom_density(aes(x=Right)) ) +
(ggplot(df_unlabel )+
geom_density(aes(x=Bottom)) ) +
(ggplot(df_unlabel )+
geom_density(aes(x=Top)) ) +
(ggplot(df_unlabel )+
geom_density(aes(x=Diagonal)) )
#Diagonal seems to have a non-normal distribution, so this can explain Clusters better, probably there is two normal distributions (clusters)
(ggplot(df )+
geom_density(aes(x=Diagonal, color=Status)) ) + theme_minimal()
#As we imagine, indeed there is 2 normal distribution explaining the classes
(ggplot(df )+
geom_density(aes(x=Length, color=Status)) ) + theme_minimal()
class <- df_unlabel[,6]
fit <- Mclust(class)
summary(fit)
plot(fit, what="BIC")
#fit_E_2$parameters
class <- df_unlabel[,6]
fit_E_2 <- Mclust(class, modelNames = c("E"), G=2)
summary(fit_E_2)
fit_E_2$parameters
#stored
fit_E_2$bic
#parameters
fit_E_2$parameters
#k = 2 (2 clusters)
#p = 1 (variable)
#the parameters are (1 class probability pi, 2 means, and 1 variance)
#from book (we can see 4 parameters)
-2*fit_E_2$loglik + log(nrow(df))*4
plot(fit_E_2, what="BIC")
plot(fit_E_2, what="density")
# add the observations using rug marks
rug(df_unlabel %>% pull(Diagonal))
class <- df_unlabel[,6]
fit_V_2 <- Mclust(class, modelNames = c("V"), G=2)
summary(fit_V_2)
fit_V_2$parameters
plot(fit_V_2, what="BIC")
plot(fit_V_2, what="density")
# add the observations using rug marks
rug(df_unlabel %>% pull(Diagonal))
fit_V_2$parameters
# 1 class probability (pi)
# 2 means
# 2 variances
#deviance = -2 * log(l)
-2*fit_E_2$loglik
-2*fit_V_2$loglik
fit_E_2$bic
fit_V_2$bic
fit_multi <- Mclust(df_unlabel)
summary(fit_multi)
plot(fit_multi, what="BIC")
fit_multi$parameters
# 3 clusters * 6 variables = 18 mean parameters
fit_multi$parameters$mean
# 3 clusters * 6 variables = 18 mean parameters
fit_multi_VVV <- Mclust(df_unlabel, G=2, modelNames = c("VVV"))
plot(fit_multi_VVV, what="density")
ggplot(df, aes(Left, Right, color=as_factor(fit_multi_VVV$classification), size=fit_multi_VVV$uncertainty))+geom_jitter()
#now with the best separation between features
ggplot(df, aes(Bottom, Diagonal, color=as_factor(fit_multi_VVV$classification), size=fit_multi_VVV$uncertainty))+geom_jitter()
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("VVV"))
fit_multi_XXX$parameters$df
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("EEE"))
fit_multi_XXX$parameters$df
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("VVE"))
fit_multi_XXX$parameters$df
fit_multi_XXX$parameters
summary(fit_multi_XXX)
fit_multi_XXX$df
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("VVV"))
fit_multi_XXX$df
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("EEE"))
fit_multi_XXX$df
fit_multi_XXX <- Mclust(df_unlabel[,1:2], G=3, modelNames = c("VVE"))
fit_multi_XXX$df
