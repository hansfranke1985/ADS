{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "1.\tThe data is divided over two tables, which is inconvenient for doing analysis. Using the merge function, merge them into one data frame using the Accident_Index field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: read the first csv file \n",
    "dfA = pd.read_csv('Accidents_2015.csv', header = 0,\n",
    "                 quotechar='\"',sep=\",\",\n",
    "                 na_values = ['na', '-', '.', ''], low_memory=False)\n",
    "# TODO: read the first csv file \n",
    "dfC = pd.read_csv('Casualties_2015.csv', header = 0,\n",
    "                 quotechar='\"',sep=\",\",\n",
    "                 na_values = ['na', '-', '.', ''], low_memory=False)\n",
    "\n",
    "# TODO: Perform the merge \n",
    "df_merged = pd.merge(dfA, dfC, left_on='Accident_Index', \n",
    "                        right_on = 'Accident_Index')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tThe \"Accident_Severity\" variable needs to be recoded. You need to replace the code by: 1=Minor, 2=Medium, 3=Severe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_rank = ['Minor', 'Medium', 'Severe']\n",
    "dfA_2 = dfA.copy()\n",
    "for i in range(len(severity_rank)):\n",
    "    j = i + 1\n",
    "    dfA_2.loc[dfA_2['Accident_Severity'] == j, 'Accident_Severity'] = severity_rank[i]\n",
    "dfA_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tFor a set of variables, missing values has been replaced by -1. Detect these values and report the names of the columns in each table that contain such values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA_3 = dfA.copy()\n",
    "for col in dfA_3.columns:\n",
    "    dfA_3.loc[dfA_3[col] == -1] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tFor all variables, check if there are any clearly extreme values, or values that do not belong in that column. If you find any, remove these records from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTypeSeries = dfA.dtypes\n",
    "continuous_var = list()\n",
    "for col_idx in range(len(dfA.columns)):\n",
    "    if (dataTypeSeries[col_idx] == \"float64\"):\n",
    "        continuous_var.append(dfA.columns[col_idx])\n",
    "continuous_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: for each column except the label column, compute the standard deviation of the columns\n",
    "# report all the values that are at distance > 3 * std from the mean value as outliers.\n",
    "def stat_outliers (col):\n",
    "    outliers = set()\n",
    "    m = col.mean()\n",
    "    s = col.std()\n",
    "    for p in col.unique():\n",
    "        if (abs(p-m) > 3 * s):\n",
    "            outliers.add(p)\n",
    "    return outliers\n",
    "\n",
    "for col in continuous_var:\n",
    "    print(\"Outliers in column \", dfA[col], \" are: \", stat_outliers(dfA[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tCompute a variable called is_minor , that checks whether a casualty was a minor or an adult. Being adult is defined as having an age of 18 or above. The column should only contain the values ‘Yes’ and ‘No’. The field name is \"Age_of_Casualty\" in the \"Casualties_2015.csv\" table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC_4 = dfC.copy()\n",
    "dfC_4.loc[dfC_4['Age_of_Casualty'] < 18, 'is_Minor'] = \"Yes\"\n",
    "dfC_4.loc[dfC_4['Age_of_Casualty'] >= 18, 'is_Minor'] = \"No\"\n",
    "dfC_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tThe 'Location_Easting_OSGR' variable has about 27 of missing values - solve this with imputation of the average of the 'Location_Easting_OSGR' of all records. That means, calculate the average of all the available values in 'Location_Easting_OSGR' and fill the missing cells in the column with the average value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, use the dataset pid.csv. This dataset were publicly available but it has been removed from the repository so use it for this assignment and don’t redistribute it. The name of the table and the names of the columns has been changed to anonymize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pid = pd.read_csv('pid.csv', header = 0,\n",
    "                 quotechar='\"',sep=\",\",\n",
    "                 na_values = ['na', '-', '.', ''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tRemove the disguised values from the table -- We need to remove the values that equal 0 from columns C,D and F as these are missing values but they have been disguised by the value 0. Remove the value but keep the record (i.e.) change the value to null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pid_1 = df_pid.copy()\n",
    "for col in ['C', 'D', 'F']:\n",
    "    df_pid_1.loc[df_pid_1[col] == 0] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tRemove the Label column and remove one of the columns if their correlation is greater than 0.5. That is, if there are two columns with correlation value > 0.5 then remove one of them and keep the other. The input for this step is the original dataframe not the one that has been produced at step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pid_cor = df_pid.copy()\n",
    "df_pid_cor = df_pid_cor[df_pid_cor.columns[0:8]]\n",
    "corr_mat = df_pid_cor.corr()\n",
    "corr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the columns that have correlation greater than 0.5 \n",
    "# and store the name of one of them in the extra_cols list\n",
    "extra_cols = list()\n",
    "for i in range(len(df_pid_cor.columns)):\n",
    "    j = 0\n",
    "    while (j < i):        \n",
    "        if (corr_mat.iloc[j, i] >= 0.5):\n",
    "            f_col_name = corr_mat.columns[j]            \n",
    "            extra_cols.append(f_col_name)\n",
    "        j = j + 1\n",
    "\n",
    "# Now, extract the columns other than the redundant ones\n",
    "new_df_cols = list(set(df_pid_cor.columns).difference(set(extra_cols)))\n",
    "new_df = df_pid_cor[new_df_cols]\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tUse a Python or R-library to find the principal components and project the data on those components. Plot the projected data on the first and the second (principal components) PCs as a scatter plot. If you are working with R, use the (prcomp) R-function. For Python, check https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_pid.columns[:8]\n",
    "# Separating out the features\n",
    "x = df_pid.loc[:, features].values\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1 = PCA(n_components = 8)               # You can also use pca = PCA(2)\n",
    "pcs1 = pca1.fit_transform(x)\n",
    "pcsDF1 = pd.DataFrame(data = pcs1, columns = ['PC'+str(i+1) for i in range(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcsDF_red = pcsDF1[['PC1', 'PC2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors1, eigenvalues1 = pca1.components_, pca1.explained_variance_\n",
    "plt.bar(np.array(range(8)), eigenvalues1, color = 'green')\n",
    "plt.xticks(np.array(range(8)), (['PC'+str(i+1) for i in range(8)]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(pcsDF1[\"PC1\"], pcsDF1[\"PC2\"])\n",
    "K = 2\n",
    "mu = pcs1.mean(axis=0)\n",
    "\n",
    "i = 1\n",
    "for axis, color in zip(eigenvectors1[:K], [\"red\",\"green\"]):\n",
    "#     start, end = mu, mu + sigma * axis ### leads to \"ValueError: too many values to unpack (expected 2)\"\n",
    "\n",
    "    # So I tried this but I don't think it's correct\n",
    "    start, end = (mu)[:K], (mu + 2 * eigenvalues1[i-1] * axis)[:K]\n",
    "    pc = 'PC'+str(i)\n",
    "    ax.arrow(start[0], start[1], end[0], end[1], head_width=0.2, head_length=0.3, fc = color, ec=color)\n",
    "    ax.annotate(pc, (end[0] + 0.05 * eigenvalues1[i-1], end[1] + 0.05 * eigenvalues1[i-1]),fontsize=14)\n",
    "    i += 1\n",
    "\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
