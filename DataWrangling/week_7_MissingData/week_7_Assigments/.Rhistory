#Original
df_fdgs <- fdgs
#df without n/a
df_fdgs_cc <- na.omit(df_fdgs
#df with mean imputation
imp_mean <- mice(df_fdgs, method = "mean", m = 1, maxit = 1)
#Original
df_fdgs <- fdgs
#df without n/a
df_fdgs_cc <- na.omit(df_fdgs)
#df with mean imputation
imp_mean <- mice(df_fdgs, method = "mean", m = 1, maxit = 1)
df_fgds_mean <- complete(imp_mean)
#look for df`s
md.pattern( df_fgds_mean)
na_idx <- is.na(df$wgt)
na_idx
df <- fdgs
na_idx <- is.na(df$wgt)
na_idx
df <- fdgs
na_idx <- is.na(df$wgt)
pred_wgt <- predict(imp_model, newdata = df[na_idx,])
df <- fdgs
na_idx <- is.na(df$wgt)
imp_model <- lm(wgt ~ reg + age + I(age^2) + sex + hgt, data = df, na.action = na.omit)
pred_wgt <- predict(imp_model, newdata = df[na_idx,])
pred_wgt
df <- fdgs
na_idx <- is.na(df$wgt)
imp_model <- lm(wgt ~ reg + age + I(age^2) + sex + hgt, data = df, na.action = na.omit)
pred_wgt <- predict(imp_model, newdata = df[na_idx,])
pred_wgt
# we can impute the predicted values like so:
df$wgt[na_idx] <- pred_wgt
# mean estimate with regression imputation:
mean(df$wgt)
df <- fdgs
na_idx <- is.na(df$wgt)
imp_model <- lm(wgt ~ reg + age + I(age^2) + sex + hgt, data = df, na.action = na.omit)
pred_wgt <- predict(imp_model, newdata = df[na_idx,])
pred_wgt
# we can impute the predicted values like so:
df$wgt[na_idx] <- pred_wgt
# mean estimate with regression imputation:
mean(df$wgt)
# mean estimate with complete case analysis:
mean(df$wgt[-na_idx])
na_idx
-na_idx
na_idx
!na_idx
# mean estimate with complete case analysis:
mean(df$wgt[!na_idx])
na_idx
-na_idx
!na_idx
!na_idx[1:10]
na_idx[1:10]
#Same thing with diferent code
df4 <- df_fdgs_age2 %>%
na_idx <- is.na(df$wgt)
#Same thing with diferent code
df4 <- df_fdgs_age2
na_idx <- is.na(df$wgt)
imp_model <- lm(wgt ~ reg + age + I(age^2) + sex + hgt, na.action = na.omit)
#Same thing with diferent code
df4 <- df_fdgs_age2
na_idx <- is.na(df$wgt)
imp_model <- lm(wgt ~ reg + age + I(age^2) + sex + hgt, data= df4, na.action = na.omit)
pred_wgt <- predict(imp_model, newdata = df4[na_idx,])
pred_wgt
# we can impute the predicted values like so:
df4$wgt[na_idx] <- pred_wgt
#Same thing with diferent code
df4 <- df_fdgs_age2
na_idx <- is.na(df4$wgt)
imp_model <- lm(wgt ~ reg + age + I(age^2) + sex + hgt, data= df4, na.action = na.omit)
pred_wgt <- predict(imp_model, newdata = df4[na_idx,])
pred_wgt
# we can impute the predicted values like so:
df4$wgt[na_idx] <- pred_wgt
#Same thing with diferent code
df4 <- df_fdgs_age2
na_idx <- is.na(df4$wgt)
imp_model <- lm(wgt ~ reg + age + I(age^2) + sex + hgt, data= df4, na.action = na.omit)
pred_wgt <- predict(imp_model, newdata = df4[na_idx,])
pred_wgt
# we can impute the predicted values like so:
df4$wgt[na_idx] <- pred_wgt
#Same thing with diferent code
df4 <- df_fdgs_age2
na_idx <- is.na(df4$wgt)
imp_model <- lm(wgt ~ reg + age + I(age^2) + sex + hgt, data= df4, na.action = na.omit)
pred_wgt <- predict(imp_model, newdata = df4[na_idx,])
pred_wgt
# we can impute the predicted values like so:
df4$wgt[na_idx] <- pred_wgt
mean(df4$wgt)
mean(df3$wgt)
# Use fit to predict the value
df3 <- df_fdgs_age2 %>%
#create a pred column to compare with original
mutate(pred = predict(multi.fit, .)) %>%
# Replace NA with pred in var1
mutate(wgt = ifelse(is.na(wgt), pred, wgt))
# See the result
head(df3)
df4
# See the result
df3[na_idx]
# Use fit to predict the value
df3 <- df_fdgs_age2 %>%
#create a pred column to compare with original
mutate(pred = predict(multi.fit, .)) %>%
# Replace NA with pred in var1
mutate(wgt = ifelse(is.na(wgt), pred, wgt))
# See the result
df3[na_idx,]
df4[na_idx,]
library(tidyverse)
library(mice)
head(mice::nhanes)
df <- nhanes
view(df)
is.na(df)
#Missing values in all columns
colSums(is.na(df))
# % of Missing values in all columns
colSums(is.na(df))/nrow(df)*100
#Put data in the DF
df_missing <- tibble(
nmissing = colnames(is.na(df)),
perc_col = colSums(is.na(df))/nrow(df)*100
)
df_missing
#Plotting as a barplot
ggplot(df_missing, aes(x=nmissing, y = perc_col) ) +
geom_bar(stat = "identity") +
labs(y="% Of missing values") +
theme_classic()
df %>%
group_by(age)%>%
summarise_all(function(x) sum(is.na(x))/n()*100)%>%
round(2)
md.pattern(df)
# Others packages to try :)
#install.packages("skimr")
skimr::skim(df)
visdat::vis_dat(df)
#Lookinf for patterns of missing data
md.pattern(fdgs)
#% of missing
sum(is.na(fdgs))/(nrow(fdgs)*ncol(fdgs))*100
# mean( df$wgt) and mean(df$hgt).
#Assign the fgds to a another df (dosent lose information later)
imp_mean <- mice(fdgs, method = "mean", m = 1, maxit = 1)
fdgs_nomiss <- complete(imp_mean)
#CHECK FINAL RESULT
colSums(is.na(fdgs_nomiss))
md.pattern(fdgs_nomiss)
#loading libraries
library(tidyverse)
library(mice)
library(dplyr)
df <- nhanes
df_cc <- na.omit(df) #omiting the n/a values
df_cc
#compute the mean
imp_mean <- mice(df, method = "mean", m = 1, maxit = 1)
#put on new df to store the missing values as mean values of each colum
df_mean <- complete(imp_mean)
df_mean
var(df)
#Comparison var ( As u can imagine if u are inputing values as MEAN, the variance will decrease because more points will become "closer to the mean")
var(df_cc)
var(df_mean)
#loading and look for summaries
md.pattern(fdgs)
df <- fdgs
na_idx <- is.na(df$wgt)
imp_model <- lm(wgt ~ reg + age + I(age^2) + sex + hgt, data = df, na.action = na.omit)
pred_wgt <- predict(imp_model, newdata = df[na_idx,])
pred_wgt
# we can impute the predicted values like so:
df$wgt[na_idx] <- pred_wgt
# mean estimate with regression imputation:
mean(df$wgt)
# mean estimate with complete case analysis:
mean(df$wgt[!na_idx])
na_idx[1:10]
!na_idx[1:10]
#Original
df_fdgs <- fdgs
#df without n/a
df_fdgs_cc <- na.omit(df_fdgs)
#df with mean imputation
imp_mean <- mice(df_fdgs, method = "mean", m = 1, maxit = 1)
df_fgds_mean <- complete(imp_mean)
#look for df`s
md.pattern( df_fgds_mean)
#df with reg manually (u have to use df without the n/a)
df_fdgs_age2 <- df_fdgs %>%
mutate( age2 = age ** 2) # elevate age to quadratic as question suggests
multi.fit = lm(wgt~reg+age+age2+sex+hgt, data=df_fdgs_age2)
summary(multi.fit)
#predict some values
value <- data.frame(
reg = c("North"),
age = 10,
age2 = c(100),
sex = c("girl"),
hgt = c(30)
)
pred2 <- predict(multi.fit, value)
pred2
# Use fit to predict the value
df3 <- df_fdgs_age2 %>%
#create a pred column to compare with original
mutate(pred = predict(multi.fit, .)) %>%
# Replace NA with pred in var1
mutate(wgt = ifelse(is.na(wgt), pred, wgt))
# See the result
df3[na_idx,]
#loading libraries
library(tidyverse)
library(mice)
# this with() notation will come in handy later
fit <- with(nhanes, lm(bmi ~ age))
#look for statistics and missing values (# 9 observations were deleted)
summary(fit)
#Let see the % of missing values in BMI colum 36%, so we can conclude this probably have a huge impact!
#% missing
colSums(is.na(nhanes))/nrow(nhanes)*100
df_nhanes <- nhanes #store in df to avoid changes in original df
imp_mean <- mice(df_nhanes, method = "mean", maxit = 1,
m = 1, print = FALSE)
xyplot(imp_mean, bmi ~ age) #plot the inserted points (red ones) WHy there is only 3 points if total missing points is 9? A: because the points inserted is always the same, plus the fact that we have only 3 "classes of age"
#look where is the missing points
df_nhanes[rowSums(is.na(df_nhanes)) > 0,]
#complete DF with missing values
df_nhanes_mean <- complete(imp_mean)
summary(df_nhanes_mean)
#Original regression without mean imputation
summary(fit)
# Fitting with mean_imputation
fit_mean <- with(df_nhanes_mean, lm(bmi ~ age))
#look for statistics=> We conclude that STD erros decrease, the p-value descrease as well but in a lesser scale!
summary(fit_mean)
#create a df to store the imputaiton
df_nhanes_reg <- df_nhanes
# Use regression (fit_mean) to predict the value
df_nhanes_reg <- df_nhanes_reg %>%
#create a pred column to compare with original
mutate(pred = predict(fit_mean, .)) %>%
# Replace NA with pred in var1
mutate(bmi = ifelse(is.na(bmi), pred, bmi))
df_nhanes_reg
#original df
summary(df_nhanes)
#df with imputation by regression => the value of mean is higher
summary(df_nhanes_reg)
#df with stocasthic regression imputation Using mice
df_nhanes_stocreg <- df_nhanes
imp_stoc <- mice(df_nhanes, method = "norm.nob", seed = 1,
m = 5, print = FALSE)
xyplot(imp_stoc, bmi ~ age )
#why there is much more red points than original 9 missing? A: Because we are using m =5, so there is 5*9 imputation points
#complete the df with imputation values (stoc regression)
df_nhanes_stocreg <- complete(imp_stoc)
#statistics of data frame
summary(df_nhanes_stocreg)
#statics of regression model => Error and p value decrease much more than original regression.
stoc_reg <- with(df_nhanes_stocreg, lm(bmi ~age))
summary(stoc_reg)
#Original regression with mean values
summary(fit_mean)
imp_stoc123 <- mice(df_nhanes, method = "norm.nob", seed = 123,
print = FALSE)
df_nhanes_stoc123 <- df_nhanes #copy original
df_nhanes_stoc123 <- complete(imp_stoc123) #complete with imputation stock seed = 123
reg123 <- with(df_nhanes_stoc123, lm(age ~ bmi)) #run the regression with this imputaiton values
summary(reg123)
imp <- mice(nhanes, m = 20, print = FALSE)
summary(imp)
imp$data
imp$imp
df_nhanes_multi <- nhanes
df_nhanes_multi <- complete(imp)
df_nhanes_multi
reg_multi <- with(df_nhanes_multi, lm(bmi ~ age))
summary(reg_multi)
summary(df_nhanes_multi)
#The pool() function combines the estimates from m repeated complete data analyses.
#Imp are already done but show again just to make code clear | Every time u run the code the results changes because there are random variance added to results by multiple imputations
imp <- mice(nhanes, m = 20, print = FALSE)
fit <- with(data = imp, exp = lm(bmi ~ age))
summary(pool(fit))
imp_stoc123 <- mice(nhanes, method = "norm.nob", seed = 123,
print = FALSE)
df_nhanes_stoc123 <- nhanes #copy original
df_nhanes_stoc123 <- complete(imp_stoc123) #complete with imputation stock seed = 123
reg123 <- with(df_nhanes_stoc123, lm(age ~ bmi)) #run the regression with this imputaiton values
summary(reg123)
?version
?version
R.Version()
imp_stoc123 <- mice(nhanes, method = "norm.nob", seed = 123, m=1
print = FALSE)
imp_stoc123 <- mice(nhanes, method = "norm.nob", seed = 123, m=1,
print = FALSE)
?
df_nhanes_stoc123 <- nhanes #copy original
df_nhanes_stoc123 <- complete(imp_stoc123) #complete with imputation stock seed = 123
reg123 <- with(df_nhanes_stoc123, lm(age ~ bmi)) #run the regression with this imputaiton values
summary(reg123)
imp_stoc123 <- mice(nhanes, method = "norm.nob", seed = 123, m=2,
print = FALSE)
?
df_nhanes_stoc123 <- nhanes #copy original
df_nhanes_stoc123 <- complete(imp_stoc123) #complete with imputation stock seed = 123
reg123 <- with(df_nhanes_stoc123, lm(age ~ bmi)) #run the regression with this imputaiton values
summary(reg123)
imp_stoc123 <- mice(nhanes, method = "norm.nob", seed = 123, m=3,
print = FALSE)
?
df_nhanes_stoc123 <- nhanes #copy original
df_nhanes_stoc123 <- complete(imp_stoc123) #complete with imputation stock seed = 123
reg123 <- with(df_nhanes_stoc123, lm(age ~ bmi)) #run the regression with this imputaiton values
summary(reg123)
imp_stoc123 <- mice(nhanes, method = "norm.nob", seed = 123, m=4,
print = FALSE)
?
df_nhanes_stoc123 <- nhanes #copy original
df_nhanes_stoc123 <- complete(imp_stoc123) #complete with imputation stock seed = 123
reg123 <- with(df_nhanes_stoc123, lm(age ~ bmi)) #run the regression with this imputaiton values
summary(reg123)
imp_stoc123 <- mice(nhanes, method = "norm.nob", seed = 123, m=4,
print = FALSE)
?
df_nhanes_stoc123 <- nhanes #copy original
df_nhanes_stoc123 <- complete(imp_stoc123) #complete with imputation stock seed = 123
reg123 <- with(df_nhanes_stoc123, lm(age ~ bmi)) #run the regression with this imputaiton values
summary(reg123)
imp_stoc123 <- mice(nhanes, method = "norm.nob", seed = 123, m=4,
print = FALSE)
?
df_nhanes_stoc123 <- nhanes #copy original
df_nhanes_stoc123 <- complete(imp_stoc123) #complete with imputation stock seed = 123
reg123 <- with(df_nhanes_stoc123, lm(age ~ bmi)) #run the regression with this imputaiton values
summary(reg123)
imp_stoc123 <- mice(nhanes, method = "norm.nob", seed = 123, m=5,
print = FALSE)
?
df_nhanes_stoc123 <- nhanes #copy original
df_nhanes_stoc123 <- complete(imp_stoc123) #complete with imputation stock seed = 123
reg123 <- with(df_nhanes_stoc123, lm(age ~ bmi)) #run the regression with this imputaiton values
summary(reg123)
imp_stoc123 <- mice(nhanes, method = "norm.nob", seed = 123, m=1,
print = FALSE)
?
df_nhanes_stoc123 <- nhanes #copy original
df_nhanes_stoc123 <- complete(imp_stoc123) #complete with imputation stock seed = 123
reg123 <- with(df_nhanes_stoc123, lm(age ~ bmi)) #run the regression with this imputaiton values
summary(reg123)
imp_stoc123 <- mice(nhanes, method = "norm.nob", seed = 123, m=1,
print = FALSE)
#df with stocasthic regression imputation Using mice
df_nhanes_stocreg <- df_nhanes
#loading libraries
library(tidyverse)
library(mice)
# this with() notation will come in handy later
fit <- with(nhanes, lm(bmi ~ age))
#look for statistics and missing values (# 9 observations were deleted)
summary(fit)
#Let see the % of missing values in BMI colum 36%, so we can conclude this probably have a huge impact!
#% missing
colSums(is.na(nhanes))/nrow(nhanes)*100
df_nhanes <- nhanes #store in df to avoid changes in original df
imp_mean <- mice(df_nhanes, method = "mean", maxit = 1,
m = 1, print = FALSE)
xyplot(imp_mean, bmi ~ age) #plot the inserted points (red ones) WHy there is only 3 points if total missing points is 9? A: because the points inserted is always the same, plus the fact that we have only 3 "classes of age"
#look where is the missing points
df_nhanes[rowSums(is.na(df_nhanes)) > 0,]
#complete DF with missing values
df_nhanes_mean <- complete(imp_mean)
summary(df_nhanes_mean)
#Original regression without mean imputation
summary(fit)
# Fitting with mean_imputation
fit_mean <- with(df_nhanes_mean, lm(bmi ~ age))
#look for statistics=> We conclude that STD erros decrease, the p-value descrease as well but in a lesser scale!
summary(fit_mean)
#create a df to store the imputaiton
df_nhanes_reg <- df_nhanes
# Use regression (fit_mean) to predict the value
df_nhanes_reg <- df_nhanes_reg %>%
#create a pred column to compare with original
mutate(pred = predict(fit_mean, .)) %>%
# Replace NA with pred in var1
mutate(bmi = ifelse(is.na(bmi), pred, bmi))
df_nhanes_reg
#original df
summary(df_nhanes)
#df with imputation by regression => the value of mean is higher
summary(df_nhanes_reg)
#df with stocasthic regression imputation Using mice
df_nhanes_stocreg <- df_nhanes
imp_stoc <- mice(df_nhanes, method = "norm.pred", seed = 1,
m = 5, print = FALSE)
#df with stocasthic regression imputation Using mice
df_nhanes_stocreg <- df_nhanes
imp_stoc <- mice(df_nhanes, method = "norm.predict", seed = 1,
m = 5, print = FALSE)
xyplot(imp_stoc, bmi ~ age )
#why there is much more red points than original 9 missing? A: Because we are using m =5, so there is 5*9 imputation points
#df with stocasthic regression imputation Using mice
df_nhanes_stocreg <- df_nhanes
imp_stoc <- mice(df_nhanes, method = "norm.predict", seed = 1,
m = 1, print = FALSE)
xyplot(imp_stoc, bmi ~ age )
#why there is much more red points than original 9 missing? A: Because we are using m =5, so there is 5*9 imputation points
#df with stocasthic regression imputation Using mice
df_nhanes_stocreg <- df_nhanes[, c("bmi, 'age")]
#df with stocasthic regression imputation Using mice
df_nhanes_stocreg <- df_nhanes[, c("bmi", "age")]
imp_stoc <- mice(df_nhanes, method = "norm.predict", seed = 1,
m = 1, print = FALSE)
xyplot(imp_stoc, bmi ~ age )
#why there is much more red points than original 9 missing? A: Because we are using m =5, so there is 5*9 imputation points
#df with stocasthic regression imputation Using mice
df_nhanes_stocreg <- df_nhanes[, c("bmi", "age")]
imp_stoc <- mice(df_nhanes_stocreg, method = "norm.predict", seed = 1,
m = 1, print = FALSE)
xyplot(imp_stoc, bmi ~ age )
#why there is much more red points than original 9 missing? A: Because we are using m =5, so there is 5*9 imputation points
#df with stocasthic regression imputation Using mice
df_nhanes_stocreg <- df_nhanes[]
imp_stoc <- mice(df_nhanes_stocreg, method = "norm.predict", seed = 1,
m = 1, print = FALSE)
xyplot(imp_stoc, bmi ~ age )
#why there is much more red points than original 9 missing? A: Because we are using m =5, so there is 5*9 imputation points
#df with stocasthic regression imputation Using mice
df_nhanes_stocreg <- df_nhanes[]
imp_stoc <- mice(df_nhanes_stocreg, method = "norm.prob", seed = 1,
m = 1, print = FALSE)
#df with stocasthic regression imputation Using mice
df_nhanes_stocreg <- df_nhanes[]
imp_stoc <- mice(df_nhanes_stocreg, method = "norm.prod", seed = 1,
m = 1, print = FALSE)
#df with stocasthic regression imputation Using mice
df_nhanes_stocreg <- df_nhanes[]
imp_stoc <- mice(df_nhanes_stocreg, method = "norm.nob", seed = 1,
m = 1, print = FALSE)
xyplot(imp_stoc, bmi ~ age )
#why there is much more red points than original 9 missing? A: Because we are using m =5, so there is 5*9 imputation points
#The pool() function combines the estimates from m repeated complete data analyses.
#Imp are already done but show again just to make code clear | Every time u run the code the results changes because there are random variance added to results by multiple imputations
imp <- mice(nhanes, m = 20, print = FALSE)
fit <- with(data = imp, exp = lm(bmi ~ age))
summary(pool(fit))
#The pool() function combines the estimates from m repeated complete data analyses.
#Imp are already done but show again just to make code clear | Every time u run the code the results changes because there are random variance added to results by multiple imputations
imp <- mice(nhanes, m = 5, print = FALSE)
fit <- with(data = imp, exp = lm(bmi ~ age))
summary(pool(fit))
#The pool() function combines the estimates from m repeated complete data analyses.
#Imp are already done but show again just to make code clear | Every time u run the code the results changes because there are random variance added to results by multiple imputations
imp <- mice(nhanes, m = 5, print = FALSE)
fit <- with(data = imp, exp = lm(bmi ~ age))
summary(pool(fit))
#The pool() function combines the estimates from m repeated complete data analyses.
#Imp are already done but show again just to make code clear | Every time u run the code the results changes because there are random variance added to results by multiple imputations
imp <- mice(nhanes, m = 20, print = FALSE)
fit <- with(data = imp, exp = lm(bmi ~ age))
summary(pool(fit))
#The pool() function combines the estimates from m repeated complete data analyses.
#Imp are already done but show again just to make code clear | Every time u run the code the results changes because there are random variance added to results by multiple imputations
imp <- mice(nhanes, m = 20, print = FALSE)
fit <- with(data = imp, exp = lm(bmi ~ age))
summary(pool(fit))
#The pool() function combines the estimates from m repeated complete data analyses.
#Imp are already done but show again just to make code clear | Every time u run the code the results changes because there are random variance added to results by multiple imputations
imp <- mice(nhanes, m = 20, print = FALSE)
fit <- with(data = imp, exp = lm(bmi ~ age))
summary(pool(fit))
#The pool() function combines the estimates from m repeated complete data analyses.
#Imp are already done but show again just to make code clear | Every time u run the code the results changes because there are random variance added to results by multiple imputations
imp <- mice(nhanes, m = 5, print = FALSE)
fit <- with(data = imp, exp = lm(bmi ~ age))
summary(pool(fit))
#The pool() function combines the estimates from m repeated complete data analyses.
#Imp are already done but show again just to make code clear | Every time u run the code the results changes because there are random variance added to results by multiple imputations
imp <- mice(nhanes, m = 5, print = FALSE)
fit <- with(data = imp, exp = lm(bmi ~ age))
summary(pool(fit))
#The pool() function combines the estimates from m repeated complete data analyses.
#Imp are already done but show again just to make code clear | Every time u run the code the results changes because there are random variance added to results by multiple imputations
imp <- mice(nhanes, m = 5, print = FALSE)
fit <- with(data = imp, exp = lm(bmi ~ age))
summary(pool(fit))
#The pool() function combines the estimates from m repeated complete data analyses.
#Imp are already done but show again just to make code clear | Every time u run the code the results changes because there are random variance added to results by multiple imputations
imp <- mice(nhanes, m = 5, print = FALSE)
fit <- with(data = imp, exp = lm(bmi ~ age))
summary(pool(fit))
#The pool() function combines the estimates from m repeated complete data analyses.
#Imp are already done but show again just to make code clear | Every time u run the code the results changes because there are random variance added to results by multiple imputations
imp <- mice(nhanes, m = 5, print = FALSE)
fit <- with(data = imp, exp = lm(bmi ~ age))
summary(pool(fit))
